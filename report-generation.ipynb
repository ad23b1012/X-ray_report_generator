{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import (\n    AutoFeatureExtractor, \n    AutoTokenizer, \n    VisionEncoderDecoderModel,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer, \n    default_data_collator,\n)\nimport pandas as pd \nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_image = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\ndf_report = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame({'imgs': [],'captions': []})\nfor i in range(len(df_image)):\n    uid = df_image.iloc[i]['uid']\n    image = df_image.iloc[i]['filename']\n    index = df_report.loc[df_report['uid'] ==uid]\n    \n    if not index.empty:    \n        index = index.index[0]\n        caption = df_report.iloc[index]['findings']\n        if type(caption) == float:\n         \n            continue \n        df = pd.concat([df,pd.DataFrame([{'imgs': image, 'captions': caption}])],axis=0)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loc = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\ndf['imgs'] = loc + df['imgs']\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\ndecoder_checkpoint = \"gpt2\"\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# maximum length for the captions\nmax_length = 128\nsample = df.iloc[99]\n\n# sample image\nimage = Image.open(sample['imgs']).convert('RGB')\n# sample caption\ncaption = sample['captions']\n\n# apply feature extractor on the sample image\ninputs = feature_extractor(images=image, return_tensors='pt')\n# apply tokenizer\noutputs = tokenizer(\n            caption, \n            max_length=max_length, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt',\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Inputs:\\n{inputs}\\nOutputs:\\n{outputs}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LoadDataset(Dataset):\n    def __init__(self, df):\n        self.images = df['imgs'].values\n        self.captions = df['captions'].values\n    \n    def __getitem__(self, idx):\n        # everything to return is stored inside this dict\n        inputs = dict()\n\n        # load the image and apply feature_extractor\n        image_path = str(self.images[idx])\n        image = Image.open(image_path).convert(\"RGB\")\n        image = feature_extractor(images=image, return_tensors='pt')\n\n        # load the caption and apply tokenizer\n        caption = self.captions[idx]\n        labels = tokenizer(\n            caption, \n            max_length=max_length, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt',\n        )['input_ids'][0]\n        \n        # store the inputs and labels in the dict we created\n        inputs['pixel_values'] = image['pixel_values'].squeeze()   \n        inputs['labels'] = labels\n        return inputs\n    \n    def __len__(self):\n        return len(self.images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\ntrain_df, valid_df = train_test_split(train_df, test_size=0.1, shuffle=True, random_state=42)\ntrain_ds = LoadDataset(train_df)\ntest_ds = LoadDataset(test_df)\nvalid_ds = LoadDataset(valid_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_checkpoint, \n    decoder_checkpoint\n    \n)\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# model.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.num_beams = 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_ds))\n\nmodel(pixel_values=batch['pixel_values'].unsqueeze(0), labels=batch['labels'].unsqueeze(0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"image-caption-generator\", # name of the directory to store training outputs\n    evaluation_strategy=\"epoch\",          # evaluate after each epoch\n    per_device_train_batch_size=16,       # batch size during training\n    per_device_eval_batch_size=16,        # batch size during evaluation\n    learning_rate=0.00005,\n    weight_decay=0.01,                    # weight decay for AdamW optimizer\n    num_train_epochs=5,                   # number of epochs to train\n    save_strategy='epoch',                # save checkpoints after each epoch\n    report_to='none',                     # prevents logging to wandb, mlflow...\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model, \n    tokenizer=feature_extractor, \n    data_collator=default_data_collator,\n    train_dataset=train_ds,\n    eval_dataset=valid_ds,\n    args=training_args,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ntorch.save(model.state_dict(), \"model.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = test_ds[43]['pixel_values']\nmodel.eval()\nwith torch.no_grad():\n    # uncomment the below line if feature extractor is not applied to the image already\n    # inputs = feature_extractor(images=inputs, return_tensors='pt').pixel_values\n\n    # model prediction \n    out = model.generate(\n        inputs.unsqueeze(0).to('cuda'), # move inputs to GPU\n        num_beams=4, \n#         max_length=17\n        )\n# convert token ids to string format\ndecoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(decoded_out)\nplt.axis('off')\nplt.imshow(torch.permute(inputs, (1, 2, 0)));","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}